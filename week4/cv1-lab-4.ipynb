{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F450%2F1*CXZ804tKLPy2hiikJbYH3w.png&f=1&nofb=1\" width=25% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Lab 4: Image Alignment, Image Stitching and Panorama Creation</font>\n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59PM, October 4, 2024 (Amsterdam time)</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\"><b>TA's: Egoitz & Roan</b></font>\n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID: 13213334\\\n",
    "Student1 Name: Jesse Wonnink\n",
    "\n",
    "Student2 ID: 15816397\\\n",
    "Student2 Name: Ruben Figge\n",
    "\n",
    "Student3 ID: 12856320\\\n",
    "Student3 Name: Chileshe Lukwesa\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment and libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import signal\n",
    "from matplotlib.patches import ConnectionPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're using the provided environment!\n",
    "assert cv2.__version__ == \"4.10.0\", \"You're not using the provided Python environment!\"\n",
    "assert np.__version__ == \"1.26.4\", \"You're not using the provided Python environment!\"\n",
    "assert matplotlib.__version__ == \"3.9.2\", \"You're not using the provided Python environment!\"\n",
    "# Proceed to the next cell if you don't get any error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instructions**\n",
    "\n",
    "Your code and discussion must be handed in this jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Lab 4 Assignment. Please also fill out your names and ID's above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "- It is mandatory to **use the Python environment provided** with the assignment; the environment specifies the package versions that have to be used to prevent the use of particular functions. Using different packages versions may lead to grade deduction. In the Python cell above you can check whether your environment is set up correctly.\n",
    "- To install the environment with the right package versions, use the following command in your terminal: ```conda env create --file=CV1_environment.yaml```, then activate the environment using the command ```conda activate cv1```.\n",
    "- Do not use additional packages or materials that have not been provided or explicitly mentioned.\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions and sub-questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "\n",
    "__Note:__ A more complete overview of the lab requirements can be found in the Course Manual on Canvas\n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations. This includes the use of generative tools such as ChatGPT.\n",
    "\n",
    "**ENSURE THAT YOU SAVE ALL RESULTS / ANSWERS ON THE QUESTIONS (EVEN IF YOU RE-USE SOME CODE).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Image Alignment (55 points)](#section-1)\n",
    "  - [Question 1 (5 points)](#question-1)\n",
    "  - [Question 2 (10 points)](#question-2)\n",
    "  - [Question 3 (15 points)](#question-3)\n",
    "  - [Question 4 (5 points)](#question-4)\n",
    "  - [Question 5 (5 points)](#question-5)\n",
    "  - [Question 6 (3 points)](#question-6)\n",
    "  - [Question 7 (5 points)](#question-7)\n",
    "  - [Question 8 (7 points)](#question-8)\n",
    "  \n",
    "- [Section 2: Image Stitching (30 points)](#section-2)\n",
    "  - [Question 9 (20 points)](#question-9)\n",
    "  - [Question 10 (5 points)](#question-10)\n",
    "  - [Question 11 (5 points)](#question-11)\n",
    "  \n",
    "- [Section 3: Automatic Panorama Stitching (15 points)](#section-3)\n",
    "  - [Question 12 (15 points)](#question-12)\n",
    "  \n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "### **Section 1: Image Alignment**\n",
    "\n",
    "In this section, you will develop a function that computes the affine transformation between two images. The goal is to align two supplied images from *Genoa Beach* by following a series of steps that involve detecting interest points, characterizing their local appearances, and matching these regions between the images. This process is fundamental in many computer vision tasks, such as object recognition, image stitching, and motion tracking.\n",
    "\n",
    "The overall procedure is outlined as follows:\n",
    "\n",
    "1. **Detect Interest Points:**\n",
    "   - Identify key points in each image that are likely to be matched between the two images.\n",
    "\n",
    "2. **Characterize Local Appearance:**\n",
    "   - Describe the local appearance around each interest point using a region descriptor.\n",
    "\n",
    "3. **Find Matches:**\n",
    "   - Match the region descriptors between the two images to identify corresponding points. You can use David Lowe's SIFT (Scale-Invariant Feature Transform) for these steps. SIFT is a powerful method for detecting and describing local features in images. For further information, refer to the [SIFT tutorial](https://docs.opencv.org/3.4.2/da/df5/tutorial_py_sift_intro.html) and the [SIFT class documentation](https://docs.opencv.org/3.4.2/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html).\n",
    "\n",
    "4. **Perform RANSAC (Random Sample Consensus):**\n",
    "   - To find the best transformation, perform the following steps:\n",
    "     - **Repeat N times:**\n",
    "       - Randomly select a set of P matched pairs from the total set of matches.\n",
    "       - Construct a matrix A and vector b using the P pairs of points. Solve for the affine transformation parameters \\((m1, m2, m3, m4, t1, t2)\\) by solving the equation \\(Ax = b\\). This equation can be solved using the pseudo-inverse \\(x = (A^T A)^{-1} A^T b\\) or by using Python's Numpy package.\n",
    "       - Apply the transformation to all matched points in the first image. If the transformation is accurate, the transformed points should lie close to their corresponding points in the second image.\n",
    "       - Count the number of inliers, where inliers are defined as points from the first image that, when transformed, lie within a radius of 10 pixels from their corresponding points in the second image.\n",
    "       - If the number of inliers exceeds the best count so far, save the transformation parameters and the set of inliers.\n",
    "\n",
    "5. **Visualize RANSAC Performance:**\n",
    "   - Plot the two images side by side and draw lines connecting the original points in the first image with their corresponding transformed points in the second image.\n",
    "\n",
    "6. **Transform the First Image:**\n",
    "   - Use the final set of transformation parameters to transform the first image. The transformed image should align with the second image. Implement this transformation using **nearest-neighbor interpolation**. Additionally, use OpenCV's built-in function `cv2.warpAffine` to compare results. Note that nearest-neighbor interpolation involves rounding the coordinates of the transformed points, which is simpler to implement than linear interpolation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-1\"></a>\n",
    "#### <font color='#FF0000'>Question 1 (5 points)</font>\n",
    "\n",
    "Create a function named **get_matching_keypoints()** that takes two images as input and returns the matching keypoints between them.\n",
    "\n",
    "The function should use the SIFT (Scale-Invariant Feature Transform) algorithm to detect and describe keypoints in both images. Once the keypoints are detected, use a brute-force matcher with Euclidean distance to find and return the matches between the two images. Make sure to convert the images to grayscale for processing.\n",
    "\n",
    "The function should return the keypoints detected in the first image, the keypoints detected in the second image, and the matches found between them. Ensure that the function prints the number of keypoints detected in each image, as well as the number of matches found.\n",
    "\n",
    "To test your function, use the images `genoa_beach.jpg` and `genoa_beach_cut_1.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_keypoints(image_1, image_2):\n",
    "    \"\"\"\n",
    "    Given two input images, find and return the matching keypoints.\n",
    "\n",
    "    Args:\n",
    "        image_1: The first image (in RGB)\n",
    "        image_2: The second image (in RGB)\n",
    "\n",
    "    Returns:\n",
    "        keypoints_image_1: Keypoints detected in the first image\n",
    "        keypoints_image_2: Keypoints detected in the second image\n",
    "        matches: Matching keypoints between the two images\n",
    "    \"\"\"\n",
    "\n",
    "    print('Finding matching features...')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Output the number of keypoints and matches\n",
    "    print(f\"Number of keypoints in the first image:  {len(keypoints_image_1)}\")\n",
    "    print(f\"Number of keypoints in the second image: {len(keypoints_image_2)}\")\n",
    "    print(f\"Number of matches: {len(matches)}\")\n",
    "\n",
    "    return keypoints_image_1, keypoints_image_2, matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-2\"></a>\n",
    "#### <font color='#FF0000'>Question 2 (10 points)</font>\n",
    "\n",
    "Create a function named `draw_random_matches` that takes the matching keypoints found in Question 1, selects a random subset of 10 matching points, and plots them on the images.\n",
    "\n",
    "The function should connect matching pairs with lines, assigning a random color to each line to make them easier to distinguish. Ensure that the keypoints are marked on both images and that the matching lines are clearly visible. The images should be displayed side by side for easy comparison.\n",
    "\n",
    "**Hint:** Use `from matplotlib.patches import ConnectionPatch` for drawing the lines between matching keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_matches(image_1, keypoints_1, image_2, keypoints_2, matches, subset_size=10):\n",
    "    \"\"\"\n",
    "    Draw a random subset of matching keypoints between two images.\n",
    "\n",
    "    Args:\n",
    "        image_1: The first image (in RGB)\n",
    "        keypoints_1: Keypoints detected in the first image\n",
    "        image_2: The second image (in RGB)\n",
    "        keypoints_2: Keypoints detected in the second image\n",
    "        matches: Matching keypoints between the two images\n",
    "        subset_size: Number of random matches to display (default is 10)\n",
    "\n",
    "    Returns:\n",
    "        None. The function displays the images with the matching keypoints.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_random_matches(image_1, keypoints_image_1, image_2, keypoints_image_2, matches, subset_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-3\"></a>\n",
    "#### <font color='#FF0000'>Question 3 (15 points)</font>\n",
    "\n",
    "In this question, you will implement the RANSAC algorithm to filter out outlier matches between two sets of keypoints by fitting a projective transformation model. You have to complete the following functions:\n",
    "\n",
    "1. **compute_projective_transformation_matrix:** This function takes a set of point correspondences and computes a projective transformation matrix using those points.\n",
    "2. **test_projective_transformation_matrix:** This function tests the computed projective transformation matrix on a set of point correspondences and identifies the inliers based on a given threshold.\n",
    "3. **ransac_algorithm:** This function applies the RANSAC algorithm to find the best projective transformation matrix by repeatedly sampling points and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projective_transformation_matrix(data):\n",
    "    \"\"\"\n",
    "    Compute a projective transformation matrix using a set of point correspondences.\n",
    "\n",
    "    Args:\n",
    "        data: An array containing point correspondences between two images.\n",
    "\n",
    "    Returns:\n",
    "        transformation_matrix: A projective transformation matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_projective_transformation_matrix(transformation_matrix, data, threshold=10):\n",
    "    \"\"\"\n",
    "    Test the projective transformation matrix on a set of point correspondences \n",
    "    and identify inliers.\n",
    "\n",
    "    Args:\n",
    "        transformation_matrix: A projective transformation matrix.\n",
    "        data: An array containing point correspondences.\n",
    "        threshold: The distance threshold to consider a transformed point as \n",
    "                   an inlier. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        inliers: A list of indices of the inliers within the input data.\n",
    "    \"\"\"\n",
    "\n",
    "    inliers = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_algorithm(keypoints_1, keypoints_2, matches, num_iterations, num_points=3, threshold=10):\n",
    "    \"\"\"\n",
    "    Apply the RANSAC algorithm to filter out outlier matches between two sets of \n",
    "    keypoints by fitting a projective transformation model.\n",
    "\n",
    "    Args:\n",
    "        keypoints_1: The keypoints from the first image.\n",
    "        keypoints_2: The keypoints from the second image.\n",
    "        matches: A list of matches between keypoints_1 and keypoints_2.\n",
    "        num_iterations: The number of iterations for the RANSAC algorithm.\n",
    "        num_points: The number of points to randomly sample in each iteration \n",
    "                    to fit the model. Default is 3.\n",
    "        threshold: The distance threshold to consider a matched pair as an \n",
    "                   inlier. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        best_transformation_matrix: The best transformation matrix found by RANSAC. \n",
    "                                    None may be returned if the matrix cannot be computed.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Print information\n",
    "    print(\"Total number of matches: \", len(matches))\n",
    "    print(\"Inliers found:           \", best_inliers)\n",
    "    print(\"Outliers removed:        \", len(matches) - best_inliers)\n",
    "\n",
    "    return best_transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30\n",
    "num_points = 3\n",
    "threshold = 10\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Print the best matrix\n",
    "print(f'\\nBest matrix:\\n{best_transformation_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-5\"></a>\n",
    "#### <font color='#FF0000'>Question 4 (5 points)</font>\n",
    "\n",
    "In this question, you will implement an affine transformation on an image using the transformation matrix obtained from the RANSAC algorithm in the previous question.\n",
    "\n",
    "Create a function named `apply_affine_transformation` that takes an image and a 2x3 affine transformation matrix as input. The function should perform the affine transformation using either forward or inverse warping, based on the specified method.\n",
    "\n",
    "Ensure that the transformed image maintains the same dimensions as the input image. The function should handle cases where the transformed coordinates go out of bounds by ignoring those pixels. You can use the resulting transformation matrix from Question 4 as the input for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_affine_transformation(image, transformation_matrix, warp_method='forward'):\n",
    "    \"\"\"\n",
    "    Perform an affine transformation on an image.\n",
    "\n",
    "    Args:\n",
    "        image: Input image, expected shape (height, width, [channels])\n",
    "        transformation_matrix: 2x3 affine transformation matrix\n",
    "        warp_method: Either 'forward' or 'inverse' for the desired warping method. Default is 'forward'.\n",
    "\n",
    "    Returns:\n",
    "        transformed_image: Transformed image of the same shape as input\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-6\"></a>\n",
    "#### <font color='#FF0000'>Question 5 (5 points)</font>\n",
    "\n",
    "Create a function named **visualize_affine_transformations()** that takes the source image, reference image, and the best transformation matrix as inputs, and visualizes the results using four different methods: forward warping, inverse warping, OpenCV’s `warpAffine`, and the original reference image.\n",
    "\n",
    "The function should display the results in a 2x4 grid of subplots:\n",
    "- The first row will display the full images transformed by each method.\n",
    "- The second row will display a zoomed-in section of each transformation for closer inspection.\n",
    "\n",
    "Ensure that the subplots are well-labeled and that the zoomed-in sections provide a clear view of the differences between the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_affine_transformations(source_image, reference_image, transformation_matrix):\n",
    "    \"\"\"\n",
    "    Compare different affine transformation methods in a 2x4 grid visualization.\n",
    "\n",
    "    Args:\n",
    "        source_image: Source image to transform\n",
    "        reference_image: Reference image to compare against\n",
    "        transformation_matrix: Best transformation matrix\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a plot without returning a value.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30\n",
    "num_points = 3\n",
    "threshold = 10\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-6\"></a>\n",
    "#### <font color='#FF0000'>Question 6 (3 points)</font>\n",
    "\n",
    "How many matches are required to solve an affine transformation formulated as follows:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "x'\\\\y'\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "m_1 & m_2\\\\\n",
    "m_3 & m_4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\y\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "t_1\\\\t_2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "This equation can be rewritten as:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "x&y&0&0&1&0\\\\\n",
    "0&0&x&y&0&1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "m_1\\\\\n",
    "m_2\\\\\n",
    "m_3\\\\\n",
    "m_4\\\\\n",
    "t_1\\\\\n",
    "t_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x'\\\\y'\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$\n",
    "Ax = b, \\;\n",
    "A = \\begin{bmatrix}\n",
    "x&y&0&0&1&0\\\\\n",
    "0&0&x&y&0&1\n",
    "\\end{bmatrix}, \\;\n",
    "x = \\begin{bmatrix}\n",
    "m_1\\\\\n",
    "m_2\\\\\n",
    "m_3\\\\\n",
    "m_4\\\\\n",
    "t_1\\\\\n",
    "t_2\n",
    "\\end{bmatrix}, \\;\n",
    "b = \\begin{bmatrix}\n",
    "x'\\\\y'\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-7\"></a>\n",
    "#### <font color='#FF0000'>Question 7 (5 points)</font>\n",
    "\n",
    "What happens if we have more point matches than the minimum required for solving an affine transformation? Will the code still work, and why?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-8\"></a>\n",
    "#### <font color='#FF0000'>Question 8 (7 points)</font>\n",
    "\n",
    "How many iterations are required in RANSAC to reliably find the correct transformation parameters? Provide a detailed explanation, and use mulitple figures to support your answer.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Image Stitching**\n",
    "\n",
    "In this section, we will explore the process of image stitching, a technique used to combine multiple images into a single, seamless panorama. This process involves aligning and blending overlapping images to create a larger, cohesive image. By determining the geometric transformation needed to align one image with another, we can map the images to a common coordinate system. Image stitching is widely used in various applications, including photography, computer vision, and virtual reality, where creating wide-angle or high-resolution images from multiple photos is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-9\"></a>\n",
    "#### <font color='#FF0000'>Question 9 (20 points)</font>\n",
    "\n",
    "Create a function named **stitch_images_together()** that takes a pair of images as input and returns the stitched result. \n",
    "\n",
    "The function should:\n",
    "- Find the best transformation between the input images using the RANSAC algorithm.\n",
    "- Estimate the size of the stitched image.\n",
    "- Transform one image into the coordinate space of the other.\n",
    "- Combine the transformed image with the other to create a seamless panorama.\n",
    "\n",
    "Ensure that the function correctly handles both directions—transforming the first image to the coordinate space of the second, and vice versa. The final output should be well-stitched and padded rather than cropped.\n",
    "\n",
    "**Note:** You can use the `cv2.warpAffine` function to apply the transformation to the images. But you should implement the stitching process yourself **without** using any built-in stitching functions from OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images_together(image_1, image_2, transformation_matrix):\n",
    "    \"\"\"\n",
    "    Given two input images, stitch image_1 onto image_2.\n",
    "\n",
    "    Args:\n",
    "        image_1: The first image (in RGB).\n",
    "        image_2: The second image (in RGB).\n",
    "        transformation_matrix: Transformation matrix for image_1 to match image_2\n",
    "\n",
    "    Returns:\n",
    "        output_image: The stitched image combining the two input images.\n",
    "        offset_image_2: Offset of image_2 in the new output image coordinate system.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return output_image, offset_image_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-10\"></a>\n",
    "#### <font color='#FF0000'>Question 10 (5 points)</font>\n",
    "\n",
    "Write code to load the images `genoa_beach_cut_2.jpg` and `genoa_beach_cut_3.jpg`, stitch them together, and visualize the results. First, stitch `genoa_beach_cut_2.jpg` onto `genoa_beach_cut_3.jpg`, then perform the reverse operation by stitching `genoa_beach_cut_3.jpg` onto `genoa_beach_cut_2.jpg`. Visualize both original images and the resulting stitched images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-11\"></a>\n",
    "#### <font color='#FF0000'>Question 11 (5 points)</font>\n",
    "\n",
    "Now, write code to load the images `sciencepark_1.jpg` and `sciencepark_2.jpg`, stitch them together, and visualize the results. First, stitch `sciencepark_1.jpg` onto `sciencepark_2.jpg`, then perform the reverse operation by stitching `sciencepark_2.jpg` onto `sciencepark_1.jpg`. Visualize both original images and the resulting stitched images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: Automatic Panorama Stitching**\n",
    "\n",
    "In this section, you will create a panorama by stitching together multiple images. The goal is to combine a series of images into a single, wide-angle view that captures the entire scene seamlessly. This process involves detecting features in each image, matching these features between consecutive images, and computing the geometric transformations needed to align and blend them into a cohesive panorama. By mapping the images to a common coordinate system using these transformations, you can effectively stitch them together into one continuous view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-12\"></a>\n",
    "#### <font color='#FF0000'>Question 12 (15 points)</font>\n",
    "\n",
    "Create a function named **`automatic_panorama_stitching(images, use_mask=True)`** that stitches a series of images together to create a panorama. The function should accept a list of input images and manually implement the stitching process using feature detection and matching techniques. This includes performing feature matching between images, estimating the homography between matched features, and warping images to align them into a panorama.\n",
    "\n",
    "The function should handle cases where stitching fails, providing appropriate error messages if necessary. After stitching, and based on the **`use_mask`** parameter, the function should crop the stitched image using the bounding rectangle to remove unnecessary areas, such as black borders.\n",
    "\n",
    "Apply your function on the image paths provided below and visualize the results for each set of images. Make sure to **also** visualize the difference between **`use_mask`** set to **True** and **False**.\n",
    "\n",
    "```python\n",
    "image_paths = [\n",
    "    'panorama_images/panorama_cut_1.jpg',\n",
    "    'panorama_images/panorama_cut_2.jpg',\n",
    "    'panorama_images/panorama_cut_3.jpg',\n",
    "    'panorama_images/panorama_cut_4.jpg',\n",
    "]\n",
    "\n",
    "image_paths = [\n",
    "    'panorama_images/panorama_2_cut_1.jpg',\n",
    "    'panorama_images/panorama_2_cut_2.jpg',\n",
    "    'panorama_images/panorama_2_cut_3.jpg',\n",
    "    'panorama_images/panorama_2_cut_4.jpg',\n",
    "    'panorama_images/panorama_2_cut_5.jpg',\n",
    "    'panorama_images/panorama_2_cut_6.jpg',\n",
    "    'panorama_images/panorama_2_cut_7.jpg',\n",
    "    'panorama_images/panorama_2_cut_8.jpg',\n",
    "]\n",
    "```\n",
    "\n",
    "**Note:** You are allowed to use any of the functions you have implemented in the previous sections, but you are also allowed to use any other functions from OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatic_panorama_stitching(images, use_mask=True):\n",
    "    \"\"\"\n",
    "    Automatically stitch a series of images together to create a panorama.\n",
    "\n",
    "    Args:\n",
    "        images: A list of input images to stitch together.\n",
    "        use_mask: A boolean indicating whether to use a mask for the final stitched image. (default is True)\n",
    "\n",
    "    Returns:\n",
    "        stitched: The final stitched image.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return stitched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "| Jesse | 40 % | 30 % | 30 % |\n",
    "| Ruben | 30 % | 40 % | 30 % |\n",
    "| Chileshe | 30 % | 30 % | 40 % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
