{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-39364-9_5/MediaObjects/489541_1_En_5_Fig3_HTML.png\" width=50% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Lab 2: Neighborhood Processing, Low-level Filters & Applications in image processing</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59, September 20, 2024 (Amsterdam time)</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\"><b>TA's: Ronny, Owen, Oliver</b></font>\n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  \\\n",
    "Student1 Name: \n",
    "\n",
    "Student2 ID: \\\n",
    "Student2 Name: \n",
    "\n",
    "Student3 ID: \\\n",
    "Student3 Name: \n",
    "\n",
    "( Student4 ID: \\\n",
    "Student4 Name: )\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment and libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're using the provided environment!\n",
    "assert cv2.__version__ == \"4.10.0\", \"You're not using the provided Python environment!\"\n",
    "assert np.__version__ == \"1.26.4\", \"You're not using the provided Python environment!\"\n",
    "assert matplotlib.__version__ == \"3.9.2\", \"You're not using the provided Python environment!\"\n",
    "# Proceed to the next cell if you don't get any error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instructions**\n",
    "\n",
    "Your code and discussion must be handed in this jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Lab 2 Assignment. Please also fill out your names and ID's above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "- It is mandatory to **use the Python environment provided** with the assignment; the environment specifies the package versions that have to be used to prevent the use of particular functions. Using different packages versions may lead to grade deduction. In the Python cell above you can check whether your environment is set up correctly.\n",
    "- To install the environment with the right package versions, use the following command in your terminal: ```conda env create --file=CV1_environment.yaml```, then activate the environment using the command ```conda activate cv1```.\n",
    "- Do not use additional packages or materials that have not been provided or explicitly mentioned.\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions and sub-questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "\n",
    "__Note:__ A more complete overview of the lab requirements can be found in the Course Manual on Canvas\n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAsâ€™ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations. This includes the use of generative tools such as ChatGPT.\n",
    "\n",
    "**ENSURE THAT YOU SAVE ALL RESULTS / ANSWERS ON THE QUESTIONS (EVEN IF YOU RE-USE SOME CODE).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Neighborhood Processing for Image Processing (3 points)](#section-1)\n",
    "  - [Question 1 (2 points)](#question-1)\n",
    "  - [Question 2 (1 points)](#question-2)\n",
    "- [Section 2: Low-Level Gaussian Filters (11 points)](#section-2)\n",
    "  - [Question 3 (2 points)](#question-3)\n",
    "  - [Question 4 (3 points)](#question-4)\n",
    "  - [Question 5 (1 point)](#question-5)\n",
    "  - [Question 6 (2 points)](#question-6)\n",
    "  - [Question 7 (1 point)](#question-7)\n",
    "  - [Question 8 (2 points)](#question-8)\n",
    "- [Section 3: Low-Level Gabors Filters (15 points)](#section-3)\n",
    "  - [Question 9 (6 points)](#question-9)\n",
    "  - [Question 10 (1 point)](#question-10)\n",
    "  - [Question 11 (1 point)](#question-11)\n",
    "  - [Question 12 (1 point)](#question-12)\n",
    "  - [Question 13 (6 points)](#question-13)\n",
    "- [Section 4: Noise in digital images (28 points)](#section-4)\n",
    "  - [Question 14 (2 points)](#question-14)\n",
    "  - [Question 15 (4 points)](#question-15)\n",
    "  - [Question 16 (1 point)](#question-16)\n",
    "  - [Question 17 (1 point)](#question-17)\n",
    "  - [Question 18 (3 points)](#question-18)\n",
    "  - [Question 19 (2 points)](#question-19)\n",
    "  - [Question 20 (2 points)](#question-20)\n",
    "  - [Question 21 (3 points)](#question-21)\n",
    "  - [Question 22 (2 points)](#question-22)\n",
    "  - [Question 23 (2 points)](#question-23)\n",
    "  - [Question 24 (3 points)](#question-24)\n",
    "  - [Question 25 (2 points)](#question-24)\n",
    "- [Section 5: Edge Detection (20 points)](#section-5)\n",
    "  - [Question 26 (4 points)](#question-26)\n",
    "  - [Question 27 (3 points)](#question-27)\n",
    "  - [Question 28 (4 point)](#question-28)\n",
    "  - [Question 29 (1 points)](#question-29)\n",
    "  - [Question 30 (2 points)](#question-30)\n",
    "  - [Question 31 (2 points)](#question-31)\n",
    "  - [Question 32 (2 points)](#question-32)\n",
    "  - [Question 33 (2 points)](#question-33)\n",
    "- [Section 6: Foreground-background separation (24 points)](#section-6)\n",
    "  - [Question 33 (8 points)](#question-33)\n",
    "  - [Question 34 (8 points)](#question-34)\n",
    "  - [Question 35 (4 points)](#question-35)\n",
    "  - [Question 36 (4 points)](#question-36)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "### **Section 1: Neighborhood Processing for Image Processing**\n",
    "\n",
    "This assignment will start with an exploration of neighborhood processing techniques, which are fundamental to image processing. These techniques allow for the extraction of structural patterns such as edges and blobs, and are extensively used in tasks like image denoising and segmentation. Neighborhood or block processing is also a key component of Convolutional Neural Networks.\n",
    "\n",
    "Neighborhood processing involves examining the pixels surrounding a point $I(x, y)$ in an image $I$ and applying a function, $h(k, l)$, that measures specific properties or relationships within that local window. This function, $h(k, l)$, is known as the neighborhood operator or local operator, and is often implemented as a linear filter.\n",
    "\n",
    "Linear filters compute a weighted sum of the neighboring pixel intensities and assign this sum to the pixel of interest, producing the output $I_{out}(i, j)$. These filters are typically represented as square matrices. Terms like filters, kernels, weight matrices, or masks are often used interchangeably in the literature. A kernel is a matrix that defines a neighborhood operation, such as edge detection or smoothing.\n",
    "\n",
    "Linear filters are applied across the entire image using operators like correlation ($\\otimes$) and convolution ($\\ast$).\\\n",
    "\n",
    "Both operators are *linear shift-invariant* (LSI), meaning they function consistently across the image. The discrete forms of these operators are expressed as follows:\n",
    "\n",
    "- **Correlation (1)**:\n",
    "\n",
    "$\\mathbf{I}_{out} = I \\otimes  \\mathbf{h} \\\\\n",
    "\\mathbf{I}_{out}(i,j) = \\sum_{k,l}  \\mathbf{I}(i+k,j+l) \\mathbf{h}(k,l)$\n",
    "\n",
    "- **Convolution (2)**:\n",
    "\n",
    "$\\mathbf{I}_{out} = \\mathbf{I} \\ast  \\mathbf{h} \\\\\n",
    "\\mathbf{I}_{out}(i,j) = \\sum_{k,l} \\mathbf{I}(i-k,j-l) \\mathbf{h}(k,l)$\n",
    "\n",
    "The following example illustrates neighborhood processing: a kernel or mask convolves over the input image, with each pixel intensity multiplied by the corresponding weight in the kernel. The example uses a $3x3$ averaging mask, showing the effect of filtering on the input image.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://assets.8thlight.com/images/insights/posts/2022-03-25-what-is-a-convolution/padding_example.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-1\"></a>\n",
    "#### <font color='#FF0000'>Question 1 (2 points)</font>\n",
    "\n",
    "What is the difference between correlation and convolution operators? How do they treat the signals $\\mathbf{I}$ and $\\mathbf{h}$?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-2\"></a>\n",
    "#### <font color='#FF0000'>Question 2 (1 point)</font>\n",
    "\n",
    "Correlation and convolution operators are equivalent when we make an assumption on the form of the mask $\\mathbf{h}$. Can you identify the case? Give precise answer here.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Low-Level Gaussian Filters**\n",
    "\n",
    "In this section, you will design common linear filters used in neighborhood processing. We will focus in on Gaussian filters, which are widely used for image smoothing and noise reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-3\"></a>\n",
    "#### <font color='#FF0000'>Question 3 (2 points)</font>\n",
    "\n",
    "The 1D Gaussian filter is defined as follows:\n",
    "\n",
    "$G_{\\sigma}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\text{ exp}(-\\frac{x^2}{2\\sigma^2})$\n",
    "\n",
    "where $\\sigma$ is the standard deviation of the Gaussian. However, such a formulation creates an infinitely large convolution kernel. In practice, the kernel is truncated with a `kernel_size` parameter such that $-\\left\\lfloor \\frac{kernel\\_size}{2}\\right\\rfloor \\leq x \\leq \\left\\lfloor \\frac{kernel\\_size}{2} \\right\\rfloor$, where $\\left\\lfloor \\cdot \\right\\rfloor$ is the floor operator. For example, if `kernel_size` equals 3, then $x \\in \\{ -1, 0, 1 \\}$.\n",
    "\n",
    "Now, implement the following function `gaussian_kernel_1D`.\n",
    "\n",
    "**Hint:** Do not forget to normalize your filter.\n",
    "\n",
    "**Note:** You are not allowed to use any Python built-in functions provided by *SciPy* or other libraries to compute the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_1D(sigma, kernel_size):\n",
    "    '''\n",
    "    Computes a 1D Gaussian kernel.\n",
    "\n",
    "    Args:\n",
    "        sigma: Standard deviation of the Gaussian distribution.\n",
    "        kernel_size: The size of the kernel, must be an odd positive integer.\n",
    "\n",
    "    Returns:\n",
    "        kernel: A 1D numpy array representing the Gaussian kernel, normalized so that the sum of all elements equals 1.\n",
    "    '''\n",
    "\n",
    "    kernel = np.zeros((1, kernel_size))\n",
    "\n",
    "    if kernel_size % 2 == 0:\n",
    "        raise ValueError(\"The kernel size must be an odd number. Otherwise, the kernel will not have a center to convolve around.\")\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gaussian_kernel_1D(2, 5)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-4\"></a>\n",
    "#### <font color='#FF0000'>Question 4 (3 points)</font>\n",
    "\n",
    "One of the most important properties of 2D Gaussian kernels is separability. This means that convolving an image with a 2D Gaussian kernel is equivalent to convolving the image twice with a 1D Gaussian filter: once along the x-axis and once along the y-axis, **separately**. A 2D Gaussian kernel can then be defined as the product of two 1D Gaussian kernels:\n",
    "\n",
    "$G_{\\sigma}(x, y) = G_{\\sigma}(x) \\times G_{\\sigma}(y)$ *(Equation A)*\n",
    "\n",
    "This is mathematically equivalent to:\n",
    "\n",
    "$G_{\\sigma}(x, y) = \\frac{1}{\\sigma^2 2\\pi}\\text{ exp}(-\\frac{x^2 + y^2}{2\\sigma^2})$ *(Equation B)*\n",
    "\n",
    "Implement the `gaussian_kernel_2D` function that corresponds to *Equation A*. You should make use of the previously implemented `gaussian_kernel_1D` function.\n",
    "\n",
    "**Note:** You are not allowed to use any Python built-in functions provided by *SciPy* or other libraries to compute the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_2D(sigma_x, sigma_y, kernel_size):\n",
    "    '''\n",
    "    Computes a 2D Gaussian kernel using the separability property.\n",
    "\n",
    "    Args:\n",
    "        sigma_x: Standard deviation of the Gaussian distribution along the x-axis.\n",
    "        sigma_y: Standard deviation of the Gaussian distribution along the y-axis.\n",
    "        kernel_size: The size of the kernel, must be an odd positive integer.\n",
    "\n",
    "    Returns:\n",
    "        kernel_2d: A 2D numpy array representing the Gaussian kernel, normalized so that the sum of all elements equals 1.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return kernel_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gaussian_kernel_2D(2, 2, 3)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-5\"></a>\n",
    "#### <font color='#FF0000'>Question 5 (1 point)</font>\n",
    "\n",
    "Consider the following two scenarios:\n",
    "\n",
    "1. An image is convolved with a 2D Gaussian kernel.\n",
    "2. The same image is convolved first with a 1D Gaussian kernel along the x-axis and then with a 1D Gaussian kernel along the y-axis.\n",
    "\n",
    "Will these two scenarios produce the same result? Explain your answer.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-6\"></a>\n",
    "#### <font color='#FF0000'>Question 6 (2 points)</font>\n",
    "\n",
    "What is the computational complexity for both scenarios described in Question 5? Provide your answer in Big O notation, using *n* for the width of the image, *m* for the height of the image, and *k* for the size of the kernel.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-7\"></a>\n",
    "#### <font color='#FF0000'>Question 7 (1 point)</font>\n",
    "\n",
    "So far, the Gaussian kernels that we have computed are primarily used for image enhancement tasks, such as denoising. However, these kernels can also be used to detect changes in image intensity, which are crucial for identifying low-level features. These features can then serve as building blocks for more complex tasks like object detection or segmentation.\n",
    "\n",
    "The first-order derivative of the 1D Gaussian kernel is given by:\n",
    "\n",
    "$\\frac{d}{dx}G_\\sigma(x)  =\\frac{d}{dx}\\left( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\right)$\n",
    "\n",
    "$ = -\\frac{x}{\\sigma^3\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$ *(Eq. C)*\n",
    "\n",
    "$ = -\\frac{x}{\\sigma^2}G_\\sigma(x)$\n",
    "\n",
    "Similarly, the first-order derivative of the 2D Gaussian kernel can be obtained by computing $\\frac{d}{dx}G_\\sigma(x,y)$ and $\\frac{d}{dy}G_\\sigma(x,y)$.\n",
    "\n",
    "Determine the first-order derivative of the 2D Gaussian kernel by computing $\\frac{d}{dx}G_\\sigma(x,y)$ and $\\frac{d}{dy}G_\\sigma(x,y)$.\n",
    "\n",
    "Show your calculations, as done above for $\\frac{d}{dx}G_\\sigma(x)$.\n",
    "\n",
    "Remember that:\n",
    "\n",
    "$G_{\\sigma}(x, y) =\\frac{1}{\\sigma^2 2\\pi}\\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-8\"></a>\n",
    "#### <font color='#FF0000'>Question 8 (2 points)</font>\n",
    "\n",
    "The second-order derivative of the Gaussian kernel can also be computed. Why is it useful to design a second-order kernel? Provide one example where it is more beneficial to use a second-order derivative Gaussian kernel compared to a first-order derivative Gaussian kernel, and explain how it can be used.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: Low-Level Gabors Filters**\n",
    "\n",
    "In this section, we will explore Gabor filters, which are widely used in image processing for edge detection, texture analysis, and feature extraction. Gabor filters are based on the Gabor wavelet, which is a sinusoidal wave modulated by a Gaussian function.\n",
    "\n",
    "Gabor filters fall into the category of linear filters and are widely used for *texture analysis*. The reason why they are a good choice for texture analysis is that they localize well in the frequency spectrum (*optimally* bandlimited) and therefore work as flexible *band-pass* filters.\n",
    "\n",
    "In the following image you can see even (cosine-modulated) and odd parts (sine-modulated) of Gabor filters with fixed-Ïƒ Gaussian. You can observe time-domain filters for the modulating sinusoidals of central frequencies, 10, 20, 30, 40 and 50 Hz, respectively.\n",
    "\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1wv6ZUOfiHMDgg0jW7n5lqkaFiFztjnEo&sz=w800\">\n",
    "\n",
    "Gabor filters with varying center frequencies are sensitive to different frequency bands. Notice that the neighboring (in the frequency spectrum) filters minimally interfere with each other.\n",
    "\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1hUQaKE_TwC-9_jMIao1CCxyjembMrUab&sz=w800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-9\"></a>\n",
    "#### <font color='#FF0000'>Question 9 (6 points)</font>\n",
    "\n",
    "To understand Gabor functions, let's start with 1D signals (e.g., speech) and then generalize to the 2D case, which is more relevant for images. A Gabor function is essentially a Gaussian function modulated by a complex sinusoidal carrier signal. This can be expressed as:\n",
    "\n",
    "$g(t) = x(t) \\cdot m(t)$ *(Equation C)*\n",
    "\n",
    "where $x(t) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{t^2}{2\\sigma^2}}$ is the Gaussian function, and $m(t) = e^{j 2 \\pi f_c t} = e^{j w_c t}$ is the complex sinusoidal carrier. Here, $\\sigma$ controls the spread of the Gaussian, and $w_c$ is the central frequency of the carrier signal.\n",
    "\n",
    "Using Euler's formula:\n",
    "\n",
    "$e^{j w t} = \\cos(w t) + j \\sin(w t)$\n",
    "\n",
    "we can rewrite the Gabor function as:\n",
    "\n",
    "$g(t) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{t^2}{2\\sigma^2}} \\left[\\cos(w_c t) + j \\sin(w_c t)\\right]$\n",
    "\n",
    "This can be further broken down into:\n",
    "\n",
    "$g(t) = g_e(t) + j g_o(t)$\n",
    "\n",
    "where $g_e(t)$ and $g_o(t)$ are the even and odd parts of the function, arranged orthogonally on the complex plane. In practice, you can use either the even or odd part for filtering purposes, or the entire complex form.\n",
    "\n",
    "In 2D, the concept extends with a sine wave described by two orthogonal spatial frequencies $u_0$ and $v_0$:\n",
    "\n",
    "$s(x,y) = \\sin(2\\pi(u_0 x + v_0 y))$\n",
    "\n",
    "and a 2D Gaussian:\n",
    "\n",
    "$C \\cdot \\exp\\left(-\\left(\\frac{(x-x_0)^2}{2\\sigma_x^2} + \\frac{(y-y_0)^2}{2\\sigma_y^2}\\right)\\right)$\n",
    "\n",
    "The 2D Gabor function then takes the following forms:\n",
    "\n",
    "$g_{\\text{real}}(x,y; \\lambda, \\theta, \\psi, \\sigma, \\gamma) = \\exp\\left(-\\frac{x^{\\prime2}+\\gamma^2 y^{\\prime2}}{2\\sigma^2}\\right) \\cos\\left( 2\\pi \\frac{x^{\\prime}}{\\lambda} + \\psi  \\right)$\n",
    "\n",
    "$g_{\\text{im}}(x,y; \\lambda, \\theta, \\psi, \\sigma, \\gamma) = \\exp\\left(-\\frac{x^{\\prime2}+\\gamma^2 y^{\\prime2}}{2\\sigma^2}\\right) \\sin\\left( 2\\pi \\frac{x^{\\prime}}{\\lambda} + \\psi  \\right)$\n",
    "\n",
    "where:\n",
    "\n",
    "$x^\\prime = x\\cos\\theta + y\\sin\\theta$\n",
    "\n",
    "$y^\\prime = -x\\sin\\theta + y\\cos\\theta$\n",
    "\n",
    "Here, $\\lambda$, $\\theta$, $\\psi$, $\\sigma$, and $\\gamma$ are parameters that control the shape and size of the Gabor function.\n",
    "\n",
    "In this question, you will design a Gabor filter bank, which is a collection of Gabor filters with different orientations and scales. To achieve this, you will have to implement the `create_gabor()` function. Before that, complete the following helper functions:\n",
    "\n",
    "- `generate_rotation_matrix()`\n",
    "- `create_cos()`\n",
    "- `create_sin()`\n",
    "- `create_gauss()`\n",
    "\n",
    "Finally, use these helper functions within the `create_gabor()` function to construct the Gabor filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotation_matrix(theta):\n",
    "    '''\n",
    "    Returns the rotation matrix for a given theta.\n",
    "    \n",
    "    Args:\n",
    "        theta: Rotation parameter in radians.\n",
    "    \n",
    "    Returns:\n",
    "        rot_matrix: Rotation matrix.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return rot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cos(rot_x, lamda, psi):\n",
    "    '''\n",
    "    Returns the 2D cosine carrier.\n",
    "    \n",
    "    Args:\n",
    "        rot_x: Rotated x-coordinates.\n",
    "        lamda: Wavelength of the cosine function.\n",
    "        psi: Phase offset.\n",
    "    \n",
    "    Returns:\n",
    "        cos_carrier: 2D cosine carrier.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return cos_carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sin(rot_x, lamda, psi):\n",
    "    '''\n",
    "    Returns the 2D sine carrier.\n",
    "    \n",
    "    Args:\n",
    "        rot_x: Rotated x-coordinates.\n",
    "        lamda: Wavelength of the sine function.\n",
    "        psi: Phase offset.\n",
    "    \n",
    "    Returns:\n",
    "        sin_carrier: 2D sine carrier.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return sin_carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gauss(rot_x, rot_y, gamma, sigma):\n",
    "    '''\n",
    "    Returns the 2D Gaussian envelope.\n",
    "    \n",
    "    Args:\n",
    "        rot_x: Rotated x-coordinates.\n",
    "        rot_y: Rotated y-coordinates.\n",
    "        gamma: Aspect ratio of the Gaussian envelope.\n",
    "        sigma: Standard deviation of the Gaussian.\n",
    "    \n",
    "    Returns:\n",
    "        gauss_env: 2D Gaussian envelope.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return gauss_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gabor(sigma, theta, lambda_, psi, gamma):\n",
    "    '''\n",
    "    Creates a complex-valued Gabor filter.\n",
    "    \n",
    "    Args:\n",
    "        sigma: Standard deviation of the Gaussian envelope.\n",
    "        theta: Orientation of the Gaussian envelope in the range [0, pi/2).\n",
    "        lambda_: Wavelength of the carrier signal.\n",
    "        psi: Phase offset for the carrier signal.\n",
    "        gamma: Aspect ratio of the Gaussian envelope.\n",
    "    \n",
    "    Returns:\n",
    "        my_gabor: A 3D array of shape [h, w, 2], where the first channel contains the real part and the second channel contains the imaginary part of the Gabor filter.\n",
    "    '''\n",
    "\n",
    "    # Set the aspect ratio.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Generate a grid\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Make sure that we get square filters.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Generate a coordinate system in the range [xmin, xmax] and [ymin, ymax].\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Convert to a 2-by-N matrix where N is the number of pixels in the kernel.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Compute the rotation of pixels by theta.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Create the Gaussian envelope.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Create the orthogonal carrier signals.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Modulate (multiply) Gaussian envelope with the carriers to compute\n",
    "    # the real and imaginary components of the complex Gabor filter.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Pack my_gabor_real and my_gabor_imaginary into my_gabor.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return my_gabor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-10\"></a>\n",
    "#### <font color='#FF0000'>Question 10 (1 point)</font>\n",
    "\n",
    "Visualize how the parameter $\\theta$ affects the Gabor filter in the spatial domain. Use the following steps:\n",
    "\n",
    "1. Initialize the parameters as follows: $\\lambda = 30$, $\\theta = 0$, $\\psi = 0$, $\\sigma = 10$, $\\gamma = 0.25$.\n",
    "2. Set $\\theta$ to $0$, $\\pi/4$, and $\\pi/2$ while keeping the other parameters unchanged.\n",
    "3. Generate and display the Gabor filters corresponding to each value of $\\theta$.\n",
    "4. Ensure that each image in the visualization is clearly labeled to indicate the value of $\\theta$ it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-11\"></a>\n",
    "#### <font color='#FF0000'>Question 11 (1 point)</font>\n",
    "\n",
    "Visualize how the parameter $\\sigma$ affects the Gabor filter in the spatial domain. Use the following steps:\n",
    "\n",
    "1. Initialize the parameters as follows: $\\lambda = 35$, $\\theta = \\pi/6$, $\\psi = \\pi/4$, $\\sigma = 10$, $\\gamma = 0.5$.\n",
    "2. Set $\\sigma$ to $10$, $30$, and $45$ while keeping the other parameters unchanged.\n",
    "3. Generate and display the Gabor filters corresponding to each value of $\\sigma$.\n",
    "4. Ensure that each image in the visualization is clearly labeled to indicate the value of $\\sigma$ it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-12\"></a>\n",
    "#### <font color='#FF0000'>Question 12 (1 point)</font>\n",
    "\n",
    "Visualize how the parameter $\\gamma$ affects the Gabor filter in the spatial domain. Use the following steps:\n",
    "\n",
    "1. Initialize the parameters as follows: $\\lambda = 25$, $\\theta = \\pi/3$, $\\psi = \\pi/6$, $\\sigma = 8$, $\\gamma = 0.75$.\n",
    "2. Set $\\gamma$ to $.25$, $.50$, and $.75$ while keeping the other parameters unchanged.\n",
    "3. Generate and display the Gabor filters corresponding to each value of $\\gamma$.\n",
    "4. Ensure that each image in the visualization is clearly labeled to indicate the value of $\\gamma$ it represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-13\"></a>\n",
    "#### <font color='#FF0000'>Question 13 (6 points)</font>\n",
    "\n",
    "Based on the visualizations of parameters and your self-study on Gabor filters, explain briefly:\n",
    "\n",
    "1. What do the parameters $\\lambda$, $\\theta$, $\\psi$, $\\sigma$, and $\\gamma$ control?\n",
    "2. How do these parameters visually influence the Gabor filter kernel?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: Noise in digital images**\n",
    "\n",
    "Digital images are often plagued by various types of noise that can significantly degrade their quality. These imperfections might originate during the image acquisition process, where sensors introduce noise, or through user errors, such as incorrect camera settings. For example, some medical imaging modalities generate low-resolution images (e.g., 128x128 pixels) that are particularly susceptible to noise. Given the exponential increase in the number of photos taken daily, enhancing noisy or corrupted images has become a critical area of research in image processing.\n",
    "\n",
    "Noise in digital images can manifest in several ways, two of the most common being salt-and-pepper noise and additive Gaussian noise.\n",
    "\n",
    "**Salt-and-Pepper Noise:** This type of noise is characterized by random occurrences of black and white pixels scattered across the image. It can result from overexposure, which creates \"hot\" pixels, or from defective sensors, leading to \"dead\" pixels. These randomly placed black and white pixels resemble the appearance of salt and pepper sprinkled on the image, hence the name.\n",
    "\n",
    "**Additive Gaussian Noise:** Another prevalent type of noise is additive Gaussian noise, often associated with thermal effects in the camera sensor. This noise is modeled by adding a random value to each pixel in the image. The random value, denoted by $\\epsilon$, is drawn from a Gaussian distribution with a mean of 0 and a standard deviation, $\\sigma$, which represents the noise level. The relationship between the original image $\\mathbf{I}(x)$ and the noisy image $\\mathbf{I}^{\\prime}(x)$ can be described by the following equation:\n",
    "\n",
    "$\\mathbf{I}^{\\prime}(x) = \\mathbf{I}(x) + \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ *(Equation D)*\n",
    "\n",
    "In this scenario, $\\mathbf{I}(x)$ represents the original image, and $\\mathbf{I}^{\\prime}(x)$ is the resulting image after Gaussian noise has been added. The noise $\\epsilon$ is sampled independently for each pixel from the Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$, where $\\sigma$ controls the intensity of the noise. As $\\sigma$ increases, the image becomes progressively noisier.\n",
    "\n",
    "Both salt-and-pepper noise and additive Gaussian noise are common in real-world imaging scenarios and require effective noise reduction techniques. In the upcoming sections, we will explore various methods for mitigating these types of noise, enhancing the quality of digital images, and ensuring that the final images are closer to their true representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-14\"></a>\n",
    "#### <font color='#FF0000'>Question 14 (2 points)</font>\n",
    "\n",
    "The Peak Signal-to-Noise Ratio (PSNR) is a commonly used metric for quantitatively evaluating the performance of image enhancement algorithms. It is derived from the Mean Squared Error (MSE), which measures the average squared difference between the original image and the enhanced (or approximated) image. The MSE is defined as:\n",
    "\n",
    "$MSE = \\frac{1}{m \\cdot n} \\sum_{x,y}\\left[\\mathbf{I}(x,y) - \\mathbf{\\hat{I}}(x,y)\\right]^2$ *(Equation E)*\n",
    "\n",
    "where $\\mathbf{I}$ is the original image of size $m \\times n$, and $\\mathbf{\\hat{I}}$ is the enhanced or approximated version of the image.\n",
    "\n",
    "The PSNR is then calculated using the MSE, and it quantifies how close the enhanced image is to the original image. The PSNR is defined as:\n",
    "\n",
    "$PSNR = 10 \\cdot \\log_{10} \\left(\\frac{\\mathbf{I}_{max}^2}{MSE}\\right) \\\\\n",
    "= 20 \\cdot \\log_{10} \\left(\\frac{\\mathbf{I}_{max}}{\\sqrt{MSE}}\\right) \\\\\n",
    "= 20 \\cdot \\log_{10} \\left(\\frac{\\mathbf{I}_{max}}{RMSE}\\right)$\n",
    "\n",
    "where $\\mathbf{I}_{max}$ is the maximum possible pixel value of the image (e.g., 255 for an 8-bit image), and $RMSE$ is the root of the MSE.\n",
    "\n",
    "Given this understanding, answer the following:\n",
    "\n",
    "When comparing different methods using the PSNR metric, is a higher value better, or is the opposite true? Explain your answer briefly.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-15\"></a>\n",
    "#### <font color='#FF0000'>Question 15 (4 points)</font>\n",
    "\n",
    "Implement the function `calculate_psnr()` to calculate the Peak Signal-to-Noise Ratio (PSNR) for both grayscale and RGB images. The function should take two images as input: the original image and its enhanced (or approximated) version.\n",
    "\n",
    "When implementing `calculate_psnr()`, ensure that it works correctly for both grayscale and RGB images. For RGB images, consider how the PSNR formula should be adjusted. Specifically, think about whether to compute the PSNR for each channel separately and then combine the results, or handle the RGB image differently.\n",
    "\n",
    "**Note:** You are not allowed to use any Python built-in functions from the *PIL* or *Skimage* libraries or any other libraries except for *NumPy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(orig_image, approx_image):\n",
    "    '''\n",
    "    Computes the PSNR (Peak Signal-to-Noise Ratio) between an original image and its enhanced version.\n",
    "\n",
    "    Args:\n",
    "        orig_image: The original image (can be grayscale or RGB).\n",
    "        approx_image: The enhanced or approximated version of the original image.\n",
    "\n",
    "    Returns:\n",
    "        psnr_value: The PSNR value in decibels (dB).\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return psnr_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-16\"></a>\n",
    "#### <font color='#FF0000'>Question 16 (1 point)</font>\n",
    "\n",
    "Using your implemented function `calculate_psnr()`, compute and print the PSNR between `frans_bauer.png` and `frans_bauer_salt_pepper.png`.\n",
    "\n",
    "**Hint:** Ensure that the data type of the images is `float32` before computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-17\"></a>\n",
    "#### <font color='#FF0000'>Question 17 (1 point)</font>\n",
    "\n",
    "Using your implemented function `calculate_psnr()`, compute and print the PSNR between `frans_bauer.png` and `frans_bauer_gaussian.png`.\n",
    "\n",
    "**Hint:** Ensure that the data type of the images is `float32` before computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-18\"></a>\n",
    "#### <font color='#FF0000'>Question 18 (3 points)</font>\n",
    "\n",
    "Design a function called `denoise_image()` to remove two types of noise: salt-and-pepper noise and Gaussian noise. The function should denoise the image by applying one of the following methods: box filtering using **cv2.blur** function, median filtering using **cv2.medianBlur** function, or Gaussian filtering using **cv2.GaussianBlur** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_image(input_image, kernel_type, kernel_size=3, sigma=1):\n",
    "    '''\n",
    "    Function denoises the image using the specified kernel type:\n",
    "    - 'box': by using a box filter\n",
    "    - 'median': by using a median filter\n",
    "    - 'gaussian': by using a gaussian filter\n",
    "    \n",
    "    Args:\n",
    "        input_image: The input image to be denoised.\n",
    "        kernel_type: A string specifying the type of filter ('box', 'median', or 'gaussian').\n",
    "        kernel_size: Size of the kernel to be used for filtering (default is 3).\n",
    "        sigma: Standard deviation for the Gaussian kernel (used only for the gaussian filter).\n",
    "\n",
    "    Returns:\n",
    "        denoised_image: The denoised image.\n",
    "    '''\n",
    "\n",
    "    if kernel_type == 'box':\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif kernel_type == 'median':\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif kernel_type == 'gaussian':\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Kernel type not implemented. Choose between \"box\", \"median\", or \"gaussian\".')\n",
    "\n",
    "    return denoised_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-19\"></a>\n",
    "#### <font color='#FF0000'>Question 19 (2 points)</font>\n",
    "\n",
    "Using your implemented function **denoise_image()**, denoise the `frans_bauer_salt_pepper.png` and `frans_bauer_gaussian.png` images by applying box filtering with kernel sizes of 3x3, 5x5, and 7x7. Display the resulting denoised images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-20\"></a>\n",
    "#### <font color='#FF0000'>Question 20 (2 points)</font>\n",
    "\n",
    "Using your implemented function **denoise_image()**, denoise the `frans_bauer_salt_pepper.png` and `frans_bauer_gaussian.png` images by applying median filtering with kernel sizes of 3x3, 5x5, and 7x7. Display the resulting denoised images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-21\"></a>\n",
    "#### <font color='#FF0000'>Question 21 (3 points)</font>\n",
    "\n",
    "Using your implemented function **my_psnr()**, compute the PSNR for each of the 12 denoised images (from Questions 19 and 20) with respect to the original `frans_bauer.png` image. Present your results in a clear and readable format **and** discuss the effect of the filter size on the PSNR. \n",
    "\n",
    "**Hint:** Ensure that the images are loaded as `float32` before computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-22\"></a>\n",
    "#### <font color='#FF0000'>Question 22 (2 points)</font>\n",
    "\n",
    "Which filter is more effective for removing salt-and-pepper noise, box or median filters? Explain your reasoning. Also, consider which filter performs better for Gaussian noise and why.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-23\"></a>\n",
    "#### <font color='#FF0000'>Question 23 (2 points)</font>\n",
    "\n",
    "Using your implemented function **denoise_image()**, denoise the `frans_bauer_gaussian.png` image by applying Gaussian filtering with kernel sizes of 3x3, 5x5, and 7x7, and standard deviations of 0.5, 1, 3, and 7.\n",
    "\n",
    "Display the resulting denoised images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-24\"></a>\n",
    "#### <font color='#FF0000'>Question 24 (3 points)</font>\n",
    "\n",
    "Present your results for the PSNR scores of the 12 images from Question 22 in a clear and readable format **and** discuss the effect of the standard deviation on the PSNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-25\"></a>\n",
    "#### <font color='#FF0000'>Question 25 (2 points)</font>\n",
    "\n",
    "What is the difference among median filtering, box filtering, and Gaussian filtering? Briefly explain how they are different at a conceptual level. If two filtering methods give similar PSNR values, can you see a qualitative difference?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "### **Section 5: Edge Detection**\n",
    "\n",
    "Edges in digital images represent areas where there is a significant change in brightness, often corresponding to the boundaries of objects. Detecting these edges is a crucial task in many computer vision applications, such as autonomous driving, where edge detection can be used to identify road boundaries and determine the vehicle's trajectory.\n",
    "\n",
    "In this section, we will explore various techniques for edge detection, focusing specifically on filters that extract the gradient of the image. The gradient represents the rate of change in brightness and is calculated using first-order derivative filters. The Sobel operator, one of the most common methods, approximates the first derivative of a Gaussian filter and is used to detect edges by highlighting the intensity gradients in an image.\n",
    "\n",
    "The Sobel operator uses two 3x3 convolution kernels, one for detecting changes in the horizontal direction (G_x) and the other for the vertical direction (G_y). The kernels are defined as follows:\n",
    "\n",
    "$G_x = \\begin{bmatrix} +1 & 0 & -1 \\\\ +2 & 0 & -2 \\\\ +1 & 0 & -1 \\end{bmatrix} * \\mathbf{I}$\n",
    "\n",
    "$G_y = \\begin{bmatrix} +1 & +2 & +1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix} * \\mathbf{I}$\n",
    "\n",
    "The gradient magnitude, which combines the horizontal and vertical gradients, is given by:\n",
    "\n",
    "$G =\\sqrt {{G_x}^2+{G_y}^2}$\n",
    "\n",
    "The direction of the gradient, indicating the orientation of the edge, is calculated as:\n",
    "\n",
    "$\\theta= \\tan^{-1}\\left(\\frac{G_y}{G_x}\\right)$\n",
    "\n",
    "These calculations allow us to detect and quantify edges in an image. In this section, we will apply these methods to extract edges from images, with a particular focus on detecting roads in still images, which is a critical task in autonomous driving systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-26\"></a>\n",
    "#### <font color='#FF0000'>Question 26 (4 points)</font>\n",
    "\n",
    "Implement the function **compute_image_gradient()** to compute the gradients of an image. This function should calculate the gradients in both the x and y directions, as well as the gradient magnitude and direction.\n",
    "\n",
    "You are not allowed to use Python built-in functions for computing gradients, but you may use the `scipy.signal.convolve2d` function for performing 2D convolution.\n",
    "\n",
    "Additionally, complete the helper function **normalize_image()** to normalize the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image: np.ndarray, low=0, high=1) -> np.ndarray:\n",
    "    '''\n",
    "    Normalizes the input image to a specified range [low, high].\n",
    "\n",
    "    Args:\n",
    "        image: The input image as a numpy array.\n",
    "        low: The lower bound of the normalization range (default is 0).\n",
    "        high: The upper bound of the normalization range (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        normalized_image: The normalized image as a numpy array.\n",
    "    '''\n",
    "\n",
    "    # Convert the image to float32 for precision during normalization\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_gradient(input_image: np.ndarray):\n",
    "    '''\n",
    "    Computes the gradients of the input image in both x and y directions,\n",
    "    along with the gradient magnitude and direction.\n",
    "\n",
    "    Args:\n",
    "        input_image: The input image as a 2D numpy array.\n",
    "\n",
    "    Returns:\n",
    "        gradient_x: The gradient of the image in the x direction.\n",
    "        gradient_y: The gradient of the image in the y direction.\n",
    "        gradient_magnitude: The magnitude of the gradient.\n",
    "        gradient_direction: The direction of the gradient (in degrees).\n",
    "    '''\n",
    "\n",
    "    # Sobel kernels for x and y directions\n",
    "    sobel_x = [[1, 0, -1],\n",
    "               [2, 0, -2],\n",
    "               [1, 0, -1]]\n",
    "\n",
    "    sobel_y = [[1, 2, 1],\n",
    "               [0, 0, 0],\n",
    "               [-1, -2, -1]]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return gradient_x, gradient_y, gradient_magnitude, gradient_direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-27\"></a>\n",
    "#### <font color='#FF0000'>Question 27 (3 points)</font>\n",
    "\n",
    "Using your implemented function **compute_image_gradient()** on `landscape.jpg`, display the following figures:\n",
    "\n",
    "1. The gradient of the image in the x-direction.\n",
    "2. The gradient of the image in the y-direction.\n",
    "3. The gradient magnitude of each pixel.\n",
    "4. The gradient direction of each pixel.\n",
    "\n",
    "Discuss what kind of information each image conveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-28\"></a>\n",
    "#### <font color='#FF0000'>Question 28 (4 points)</font>\n",
    "\n",
    "Compared to the Sobel filter, a Laplacian of Gaussian (LoG) relies on the second derivative of a Gaussian filter. Hence, it will focus on large gradients in the image.\n",
    "\n",
    "Implement the function **compute_laplacian_of_gaussian()** to compute the Laplacian of Gaussian (LoG) for an image using each the following methods:\n",
    "\n",
    "1. Smoothing the image with a Gaussian kernel, then applying the Laplacian (second derivative) to the smoothed image.\n",
    "2. Convolving the image directly with a LoG kernel.\n",
    "3. Computing the Difference of Gaussians (DoG) at different scales $\\sigma_1$ and $\\sigma_2$.\n",
    "\n",
    "The function should take arguments *method*, *kernel_size*, *sigma*, *sigma_1*, and *sigma_2* to specify the parameters for each method.\n",
    "\n",
    "**Note:** You are not allowed to use Python built-in functions for computing LoG kernels, but you can use `scipy.signal.convolve2d` for 2D convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian_of_gaussian(image, method, kernel_size=5, sigma=0.5, sigma_1=0.5, sigma_2=1.0):\n",
    "    '''\n",
    "    Computes the Laplacian of Gaussian (LoG) using the specified method.\n",
    "\n",
    "    Args:\n",
    "        image: Input image as a 2D numpy array.\n",
    "        method: Specifies which method to use (1, 2, or 3).\n",
    "        kernel_size: Size of the Gaussian kernel (default is 5).\n",
    "        sigma: Standard deviation of the Gaussian for methods 1 and 2 (default is 0.5).\n",
    "        sigma_1: Standard deviation of the first Gaussian for method 3 (default is 0.5).\n",
    "        sigma_2: Standard deviation of the second Gaussian for method 3 (default is 1.0).\n",
    "\n",
    "    Returns:\n",
    "        log_image: The resulting image after applying the Laplacian of Gaussian.\n",
    "    '''\n",
    "\n",
    "    # Define the Laplacian kernel for the second derivative\n",
    "    laplacian_kernel = np.array([[0, -1, 0],\n",
    "                                 [-1, 4, -1],\n",
    "                                 [0, -1, 0]])\n",
    "\n",
    "    if method == 1:\n",
    "        # Method 1: Gaussian smoothing followed by the Laplacian\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif method == 2:\n",
    "        # Method 2: Direct convolution with a LoG kernel\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif method == 3:\n",
    "        # Method 3: Difference of Gaussians (DoG)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 1, 2, or 3.\")\n",
    "\n",
    "    return log_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-29\"></a>\n",
    "#### <font color='#FF0000'>Question 29 (1 point)</font>\n",
    "\n",
    "Test your **compute_laplacian_of_gaussian()** function using `landscape.jpg` and visualize your results using the three methods. Use the default parameters provided for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-30\"></a>\n",
    "#### <font color='#FF0000'>Question 30 (2 points)</font>\n",
    "\n",
    "Discuss the differences between applying the three methods of the Laplacian of Gaussian (LoG) on `landscape.jpg`. Consider how each method approaches edge detection and the visual results produced by each technique.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-31\"></a>\n",
    "#### <font color='#FF0000'>Question 31 (2 points)</font>\n",
    "\n",
    "In the first method, why is it important to convolve an image with a Gaussian before convolving with a Laplacian?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-32\"></a>\n",
    "#### <font color='#FF0000'>Question 32 (2 points)</font>\n",
    "\n",
    "In the third method, what is the best ratio between $\\sigma_1$ and $\\sigma_2$ to achieve the best approximation of the LoG? What is the purpose of having two different standard deviations?\n",
    "\n",
    "**Hint**: The best approximation for our case is found when both $\\sigma_1$ and $\\sigma_2$ are within the range of 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-33\"></a>\n",
    "#### <font color='#FF0000'>Question 33 (2 points)</font>\n",
    "\n",
    "What else is needed to improve the performance and isolate the road? You don't have to provide any specific parameter or algorithm. Propose a direction that would be interesting to explore and explain how you would approach it.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "### **Section 6: Foreground-background separation**\n",
    "\n",
    "Foreground-background separation is a crucial task in the field of computer vision, enabling applications such as object recognition, scene understanding, and image editing. The objective is to distinguish the main object (foreground) from the surrounding area (background) in an image. This process is essential when the foreground object needs to be isolated for further analysis or processing.\n",
    "\n",
    "In this section, we will implement a simple unsupervised algorithm that leverages texture variations to segment the foreground object from the background. The assumption is that the foreground object has a distinct combination of textures compared to the background, which can be exploited for separation.\n",
    "\n",
    "Gabor filters, known for their effectiveness in capturing texture information due to their frequency domain characteristics, will be used to analyze the texture differences within the image. By applying a collection of Gabor filters with varying scales and orientationsâ€”referred to as a *filter bank*â€”we can effectively distinguish between different texture patterns in the image.\n",
    "\n",
    "In the example below, you can see how this process works. The **left** image shows the original input image. The **middle** image displays the foreground mask, which identifies the pixels that belong to the foreground object. The **right** image shows the result of applying the foreground mask to the input image, effectively isolating the foreground object from the background.\n",
    "\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1N4hExtMIiMxM03OK4iHKURTvSibccEwd&sz=w1000\">\n",
    "\n",
    "See the outline of the algorithm below:\n",
    "\n",
    "---\n",
    "\n",
    "**Foreground-Background Segmentation Algorithm**\n",
    "\n",
    "---\n",
    "\n",
    "**Input:** $x$ - input image\n",
    "\n",
    "**Output:** $y$ - pixelwise labels\n",
    "\n",
    "1.   Convert to grayscale if necessary.\n",
    "\n",
    ">**if** $x$ is RGB **then**\n",
    "\n",
    ">>$x$ $\\leftarrow$ rgb2gray($x$)\n",
    "\n",
    ">**end if**\n",
    "\n",
    "2.   Create Gabor filterbank, $\\mathcal{F}_{gabor}$, with varying $\\sigma$, $\\lambda$ and $\\theta$.\n",
    "\n",
    "3.   Filter $x$ with the filterbank. Store each output in $fmaps$.\n",
    "\n",
    "4.   Compute the magnitude of the complex $fmaps$. Store the results in $fmags$.\n",
    "\n",
    ">$fmags$ $\\leftarrow$  $\\vert fmaps \\vert$\n",
    "\n",
    "5.   Smooth $fmags$.\n",
    "\n",
    ">$fmags$ $\\leftarrow$  smooth($fmags$)\n",
    "\n",
    "6.   Convert $fmags$ into data matrix, $f$.\n",
    "\n",
    ">$f$ $\\leftarrow$  reshape($fmags$)\n",
    "\n",
    "7.   Cluster $f$ using kmeans into two sets.\n",
    "\n",
    ">$y$ $\\leftarrow$  kmeans($f$, 2)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-33\"></a>\n",
    "#### <font color='#FF0000'>Question 33 (7 points)</font>\n",
    "\n",
    "Please get familiar with the provided skeleton code below. You will need your implementation of the **create_gabor()** function. When you successfully implement all functions, the code should run without problems and produce a reasonable segmentation with the default parameters on **kobi.png**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_id=\"kobi\"):\n",
    "    '''\n",
    "    Load an image, resize it, and set the correct color representation.\n",
    "\n",
    "    Args:\n",
    "        image_id: ID of the image (default is \"kobi\")\n",
    "\n",
    "    Returns:\n",
    "        image: Loaded and processed image\n",
    "    '''\n",
    "    if image_id == 'clownfish':\n",
    "        image = cv2.imread('./images/clownfish.jpg')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif image_id == 'flower':\n",
    "        image = cv2.imread('./images/flower.jpg')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif image_id == 'kobi':\n",
    "        image = cv2.imread('./images/kobi.png')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif image_id == 'landscape':\n",
    "        image = cv2.imread('./images/landscape.jpg')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif image_id == 'polar_bear_hiding':\n",
    "        image = cv2.imread('./images/polar_bear_hiding.jpg')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif image_id == 'robin':\n",
    "        image = cv2.imread('./images/robin.jpg')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif image_id == 'swan':\n",
    "        image = cv2.imread('./images/swan.jpg')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Image not available.')\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=\"Kobi\", colormap='gray'):\n",
    "    '''\n",
    "    Display an image in grayscale.\n",
    "\n",
    "    Args:\n",
    "        image: Image to be displayed\n",
    "        title: Title for the image (default is \"Kobi\")\n",
    "        colormap: Colormap used by matplotlib (default is 'gray')\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(image, cmap=colormap)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Foreground-Background Segmentation Algorithm\n",
    "### Part 0 - Read Image and Convert to Grayscale\n",
    "\n",
    "def preprocess_image(image_id=\"kobi\"):\n",
    "    '''\n",
    "    Load an image, resize it, and convert it to grayscale if necessary.\n",
    "\n",
    "    Args:\n",
    "        image_id: ID of the image to load (default is \"Kobi\")\n",
    "\n",
    "    Returns:\n",
    "        grayscale_image: Grayscale image\n",
    "    '''\n",
    "    # Load the image\n",
    "    image = load_image(image_id)\n",
    "    show_image(image)\n",
    "\n",
    "    # Check if the image is grayscale or RGB\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return grayscale_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_image = preprocess_image(image_id=\"kobi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gabor_filter_bank(image):\n",
    "    '''\n",
    "    Create a Gabor filter bank with varying scales and orientations.\n",
    "\n",
    "    Args:\n",
    "        image: Grayscale input image\n",
    "\n",
    "    Returns:\n",
    "        gabor_filter_bank: A list of dictionaries containing Gabor filters and their parameters\n",
    "    '''\n",
    "    num_rows, num_cols = image.shape\n",
    "\n",
    "    # Estimate the minimum and maximum wavelengths for the sinusoidal carriers\n",
    "    lambda_min = 4 / np.sqrt(2)\n",
    "    lambda_max = np.sqrt(np.abs(num_rows) ** 2 + np.abs(num_cols) ** 2)\n",
    "\n",
    "    # Specify the carrier wavelengths\n",
    "    n = np.floor(np.log2(lambda_max / lambda_min))\n",
    "    lambdas = 2 ** np.arange(0, (n - 2) + 1) * lambda_min\n",
    "\n",
    "    # Define the set of orientations for the Gaussian envelope (Thetas)\n",
    "    delta_theta = np.pi / 4.0\n",
    "    orientations = np.arange(0, np.pi + delta_theta, delta_theta)\n",
    "\n",
    "    # Define the set of sigmas for the Gaussian envelope (spread of the Gaussian)\n",
    "    sigmas = np.array([1, 2])\n",
    "\n",
    "    gabor_filter_bank = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Total number of features\n",
    "    n_features = len(lambdas) * len(sigmas) * len(orientations)\n",
    "\n",
    "    with tqdm(total=n_features) as pbar:\n",
    "\n",
    "        # Generate Gabor filters for each combination of wavelength, sigma, and orientation\n",
    "        for wavelength in lambdas:\n",
    "            for sigma in sigmas:\n",
    "                for theta in orientations:\n",
    "                    # Filter parameter configuration\n",
    "                    psi = 0\n",
    "                    gamma = 0.5\n",
    "\n",
    "                    # Create a Gabor filter with the specified parameters\n",
    "                    filter_config = {}\n",
    "                    filter_config[\"filter_pairs\"] = create_gabor(sigma, theta, wavelength, psi, gamma)\n",
    "                    filter_config[\"sigma\"] = sigma\n",
    "                    filter_config[\"wavelength\"] = wavelength\n",
    "                    filter_config[\"theta\"] = theta\n",
    "                    filter_config[\"psi\"] = psi\n",
    "                    filter_config[\"gamma\"] = gamma\n",
    "\n",
    "                    # Append the filter configuration to the filter bank\n",
    "                    gabor_filter_bank.append(filter_config)\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "\n",
    "    creation_time = time.time() - start_time\n",
    "    \n",
    "    # Print details about the created filter bank\n",
    "    print(f'Total number of filters: {len(gabor_filter_bank)}')\n",
    "    print(f'Number of scales (sigma): {len(sigmas)}')\n",
    "    print(f'Number of orientations (theta): {len(orientations)}')\n",
    "    print(f'Number of carriers (lambda): {len(lambdas)}')\n",
    "    print(f'\\nFilter bank created in {creation_time} seconds.')\n",
    "\n",
    "    return gabor_filter_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabor_filter_bank = create_gabor_filter_bank(grayscale_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Foreground-Background segmentation algoritm\n",
    "### Part 2 - gabor features fMaps and fMags\n",
    "\n",
    "def extract_gabor_features(image, gabor_filter_bank, visualize=False):\n",
    "    '''\n",
    "    Filter images using the Gabor filter bank with quadrature pairs (real and imaginary parts).\n",
    "\n",
    "    Args:\n",
    "        image: Grayscale input image\n",
    "        gabor_filter_bank: A list of dictionaries containing Gabor filters and their parameters\n",
    "        visualize: Flag to visualize filter responses (default is False)\n",
    "\n",
    "    Returns:\n",
    "        feature_magnitudes: List of magnitude responses for each Gabor filter\n",
    "    '''\n",
    "    feature_maps = []\n",
    "\n",
    "    with tqdm(total=len(gabor_filter_bank)) as pbar:\n",
    "\n",
    "        for gabor_filter in gabor_filter_bank:\n",
    "            # Filter the image with the real and imaginary parts of the Gabor filter\n",
    "\n",
    "            real_output = None\n",
    "            imaginary_output = None\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # Append the real and imaginary parts to the list of feature maps\n",
    "            feature_maps.append(np.stack((real_output, imaginary_output), 2))\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Visualize the filter responses if requested\n",
    "            if visualize:\n",
    "                fig = plt.figure()\n",
    "                ax = fig.add_subplot(1, 2, 1)\n",
    "                ax.imshow(real_output, cmap='gray')\n",
    "                ax.set_title(f'Real Part, Î»={gabor_filter[\"wavelength\"]:.4f}, Î¸={gabor_filter[\"theta\"]:.4f}, Ïƒ={gabor_filter[\"sigma\"]:.4f}')\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                ax = fig.add_subplot(1, 2, 2)\n",
    "                ax.imshow(imaginary_output, cmap='gray')\n",
    "                ax.set_title(f'Imaginary Part, Î»={gabor_filter[\"wavelength\"]:.4f}, Î¸={gabor_filter[\"theta\"]:.4f}, Ïƒ={gabor_filter[\"sigma\"]:.4f}')\n",
    "                ax.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "    # Compute the magnitude of the output responses\n",
    "    feature_magnitudes = []\n",
    "\n",
    "    with tqdm(total=len(feature_maps)) as pbar:\n",
    "    \n",
    "        for i, feature_map in enumerate(feature_maps):\n",
    "\n",
    "            # Compute the magnitude of the feature map\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Visualize the magnitude response if requested\n",
    "            if visualize:\n",
    "                plt.figure()\n",
    "                plt.imshow(magnitude.astype(np.uint8), cmap='gray')\n",
    "                plt.title(f'Magnitude, Î»={gabor_filter_bank[i][\"wavelength\"]:.4f}, Î¸={gabor_filter_bank[i][\"theta\"]:.4f}, Ïƒ={gabor_filter_bank[i][\"sigma\"]:.4f}')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "    print(f'Created {len(feature_magnitudes)} features for each pixel')\n",
    "    return feature_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_magnitudes = extract_gabor_features(grayscale_image, gabor_filter_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Foreground-Background segmentation algoritm\n",
    "### Part 3 - cluster features in two sets\n",
    "\n",
    "def cluster_features(image, feature_magnitudes, apply_smoothing=True):\n",
    "    '''\n",
    "    Cluster pixels based on their Gabor feature magnitudes.\n",
    "\n",
    "    Hint: To standardize the features, you can use the following web page: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
    "\n",
    "    Args:\n",
    "        image: Grayscale input image\n",
    "        feature_magnitudes: List of magnitude responses for each Gabor filter\n",
    "        apply_smoothing: Flag to apply Gaussian smoothing (default is True)\n",
    "\n",
    "    Returns:\n",
    "        pixel_labels: Cluster labels for each pixel\n",
    "    '''\n",
    "    num_rows, num_cols = image.shape\n",
    "    features = np.zeros(shape=(num_rows, num_cols, len(feature_magnitudes)))\n",
    "\n",
    "    if apply_smoothing:\n",
    "        # Apply Gaussian smoothing to each feature magnitude\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    else:\n",
    "        # Directly insert magnitude images into the features matrix\n",
    "        for i, feature_magnitude in enumerate(feature_magnitudes):\n",
    "            features[:, :, i] = feature_magnitude\n",
    "\n",
    "    # Reshape the features matrix to prepare for clustering\n",
    "    features = np.reshape(features, newshape=(num_rows * num_cols, -1))\n",
    "\n",
    "    # Standardize the features   \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Apply k-means clustering\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    print(f'Clustering completed in {time.time() - start_time} seconds.')\n",
    "\n",
    "    return pixel_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_labels = cluster_features(grayscale_image, feature_magnitudes, apply_smoothing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Foreground-Background segmentation algorithm\n",
    "### Part 4 - Visualise result\n",
    "\n",
    "def visualize_clustering_results(image, pixel_labels):\n",
    "    '''\n",
    "    Visualize the clustering results and segmentation.\n",
    "\n",
    "    Args:\n",
    "        image: Grayscale input image\n",
    "        pixel_labels: Cluster labels for each pixel\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    num_rows, num_cols = image.shape\n",
    "    pixel_labels = np.reshape(pixel_labels, newshape=(num_rows, num_cols))\n",
    "\n",
    "    # Visualize pixel clusters\n",
    "    plt.figure()\n",
    "    plt.title('Pixel Clusters')\n",
    "    plt.imshow(pixel_labels)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Use pixel labels to visualize segmentation\n",
    "    segmented_image1 = np.zeros_like(image)\n",
    "    segmented_image2 = np.zeros_like(image)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    binary_mask = pixel_labels == 0 # Check for the value of your labels in pixel_labels (could be 1 or 2 instead of 0)\n",
    "\n",
    "    segmented_image1[binary_mask] = image[binary_mask]\n",
    "    segmented_image2[~binary_mask] = image[~binary_mask]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title('Montage')\n",
    "    plt.imshow(segmented_image1, cmap='gray', interpolation='none')\n",
    "    plt.imshow(segmented_image2, cmap='jet', interpolation='none', alpha=0.7)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "visualize_clustering_results(grayscale_image, pixel_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-34\"></a>\n",
    "#### <font color='#FF0000'>Question 34 (7 points)</font>\n",
    "\n",
    "Implement the `foreground_background_separation()` function using Gabor filters. Complete the function provided and experiment with different images (`clownfish`, `flower`, `kobi`, `polar_bear_hiding`, `robin` **and** `swan`) using the default parameters.\n",
    "\n",
    "Make sure to visualize the results using four subplots: the original image, the pixel representation projected onto the first principal component, the pixel clusters, and the montage showing segmentation. \n",
    "\n",
    "After visualizing the results, observe how the `sigmas` and `orientations` parameters influence the foreground-background separation. Describe your observations, focusing on the effectiveness of the separation and any patterns or trends you notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreground_background_separation(image, sigmas, orientations, apply_smoothing=True):\n",
    "    '''\n",
    "    Perform foreground-background segmentation using Gabor filters.\n",
    "\n",
    "    Args:\n",
    "        image: Grayscale input image\n",
    "        sigmas: List of sigmas for the Gaussian envelope\n",
    "        orientations: List of orientations for the Gabor filters\n",
    "        apply_smoothing: Flag to apply Gaussian smoothing to magnitude images (default is True)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control settings\n",
    "delta_theta = 2 * np.pi / 8  # Step size for orientations\n",
    "orientations = np.arange(0, np.pi + delta_theta, delta_theta)\n",
    "sigmas = np.array([1.0, 2.0])\n",
    "apply_smoothing = True\n",
    "\n",
    "image_names = [\n",
    "    \"clownfish\",\n",
    "    \"flower\",\n",
    "    \"kobi\",\n",
    "    \"polar_bear_hiding\",\n",
    "    \"robin\",\n",
    "    \"swan\"\n",
    "]\n",
    "\n",
    "# Loop through each image, load it, and apply foreground-background separation\n",
    "for image_name in image_names:\n",
    "    # Load the image\n",
    "    image = load_image(image_name)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Perform foreground-background separation\n",
    "    foreground_background_separation(image, sigmas, orientations, apply_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-35\"></a>\n",
    "#### <font color='#FF0000'>Question 35 (4 points)</font>\n",
    "\n",
    "Experiment with different `sigma` and `theta` settings for the `foreground_background_separation()` function until you achieve reasonable outputs. For each input image, report the parameter settings that work best and provide an explanation for why these settings are effective. \n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-36\"></a>\n",
    "#### <font color='#FF0000'>Question 36 (4 points)</font>\n",
    "\n",
    "After achieving decent separation on all test images, run the script again using the corresponding parameters, but this time set `apply_smoothing = False`. \n",
    "\n",
    "Describe what you observe in the output when smoothing is not applied to the magnitude images. Explain why these changes occur and discuss the reasoning behind the smoothing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
